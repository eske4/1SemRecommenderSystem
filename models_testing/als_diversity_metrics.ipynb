{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# spark imports\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "# data science imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# recommenders imports\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.notebook_utils import is_jupyter\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation, SparkDiversityEvaluation\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.utils.notebook_utils import store_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.20 (main, Oct  3 2024, 07:27:41) \n",
      "[GCC 11.2.0]\n",
      "Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"System version: {sys.version}\")\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Column names for the dataset\n",
    "COL_USER = \"user_id\"\n",
    "COL_TRACK = \"track_id\"\n",
    "COL_COUNT = \"playcount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 12:03:16 WARN Utils: Your hostname, manuel-albino-asus resolves to a loopback address: 127.0.1.1; using 192.168.87.143 instead (on interface wlp1s0)\n",
      "24/11/07 12:03:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/07 12:03:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/07 12:03:17 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\", config={'spark.local.dir': \"/home/manuel-albino/spark-temp\", 'spark.cleaner.ttl': \"true\"})\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "|user_id|track_id|playcount|\n",
      "+-------+--------+---------+\n",
      "|20     |19508   |5        |\n",
      "|121    |43554   |3        |\n",
      "+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in the dataset into pyspark DataFrame\n",
    "song_ratings = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../remappings/data/Modified_Listening_History.txt\")\n",
    "\n",
    "#remapping\n",
    "song_ratings = song_ratings.withColumn(\"track_id_temp\", song_ratings.track_id).withColumn(\"user_id_temp\", song_ratings.user_id)\n",
    "\n",
    "song_ratings = song_ratings.withColumn(\"track_id\", song_ratings.user_id_temp).withColumn(\"user_id\", song_ratings.track_id_temp)\n",
    "\n",
    "# key = old column, value = new column\n",
    "mapping = {\n",
    "    \"track_id\": COL_USER,\n",
    "    \"user_id\": COL_TRACK,\n",
    "    \"playcount\": COL_COUNT\n",
    "}\n",
    "\n",
    "song_ratings = song_ratings.select(*[F.col(old).alias(new) for old, new in mapping.items()])\n",
    "sample = song_ratings.sample(False, 0.001, 0)\n",
    "\n",
    "# show matrix (track, user, playcount)\n",
    "sample.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 7319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N test 2324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(sample, ratio=0.75, seed=123)\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS model creation with Confidence column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+------------------+\n",
      "|user_id|track_id|playcount|confidence        |\n",
      "+-------+--------+---------+------------------+\n",
      "|20     |19508   |5        |2.791759469228055 |\n",
      "|121    |43554   |3        |2.386294361119891 |\n",
      "|374    |11074   |11       |3.4849066497880004|\n",
      "|446    |8652    |4        |2.6094379124341005|\n",
      "|533    |2198    |1        |1.6931471805599454|\n",
      "|794    |1669    |1        |1.6931471805599454|\n",
      "|794    |29817   |2        |2.0986122886681096|\n",
      "|1044   |7152    |1        |1.6931471805599454|\n",
      "|1277   |28299   |1        |1.6931471805599454|\n",
      "|1566   |12773   |4        |2.6094379124341005|\n",
      "+-------+--------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 1 \n",
    "\n",
    "# Transform playcount to confidence using the current alpha\n",
    "train_with_confidence = train.withColumn(\"confidence\", 1 + alpha * F.log(1 + F.col(COL_COUNT))).drop(COL_COUNT)\n",
    "\n",
    "train_with_confidence.show(10, truncate=False)\n",
    "\n",
    "header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_TRACK,\n",
    "    \"ratingCol\": \"confidence\",\n",
    "}\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=True,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=42,\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 5.704328973999992 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = als.fit(train_with_confidence)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 12:03:45 WARN Column: Constructing trivially true equals predicate, 'user_id#47 = user_id#47'. Perhaps you need to use aliases.\n",
      "24/11/07 12:03:45 WARN Column: Constructing trivially true equals predicate, 'track_id#48 = track_id#48'. Perhaps you need to use aliases.\n",
      "[Stage 708:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72470\n",
      "Took 37.105184002000016 seconds for prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "\n",
    "    # Get the cross join of all user-item pairs and score them.\n",
    "    users = train.select(COL_USER).distinct()\n",
    "    items = train.select(COL_TRACK).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item)\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_TRACK] == train[COL_TRACK]),\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_COUNT}\"].isNull()) \\\n",
    "        .select('pred.' + COL_USER, 'pred.' + COL_TRACK, 'pred.' + \"prediction\")\n",
    "\n",
    "    # In Spark, transformations are lazy evaluation\n",
    "    # Use an action to force execute and measure the test time \n",
    "    top_all.cache().count()\n",
    "\n",
    "\n",
    "    # top k recommendations for each user\n",
    "    window = Window.partitionBy(COL_USER).orderBy(F.col(\"prediction\").desc())    \n",
    "\n",
    "    top_k_reco = top_all.select(\"*\", F.row_number().over(window).alias(\"rank\")).filter(F.col(\"rank\") <= TOP_K).drop(\"rank\")\n",
    "    \n",
    "\n",
    "    print(top_k_reco.count())\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+\n",
      "|user_id|track_id|    prediction|\n",
      "+-------+--------+--------------+\n",
      "|     20|    2909| 2.2323916E-26|\n",
      "|     20|    2926| 1.3198016E-26|\n",
      "|     20|    4069|  6.238957E-20|\n",
      "|     20|    4304| 1.7561498E-26|\n",
      "|     20|    4916| 2.0033882E-27|\n",
      "|     20|    5365| 1.1807414E-26|\n",
      "|     20|    7844|-2.9071954E-23|\n",
      "|     20|   11109|-1.3385629E-24|\n",
      "|     20|   11839|  -2.60211E-26|\n",
      "|     20|   12386| -5.672136E-20|\n",
      "|     20|   12565| 1.3104992E-26|\n",
      "|     20|   13098| 1.2525717E-26|\n",
      "|     20|   15321| 1.4147011E-26|\n",
      "|     20|   22071| 6.4944994E-12|\n",
      "|     20|   23142| 2.3834708E-26|\n",
      "|     20|   24899|-3.6086246E-23|\n",
      "|     20|   28345|  1.823058E-26|\n",
      "|     20|   29650| 5.2910504E-22|\n",
      "|     20|   30197| 1.1842804E-21|\n",
      "|     20|   39566| 2.3820096E-26|\n",
      "+-------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Ranking and Diversity functions\n",
    "Usable for pySpark. To test with bpr need to use recommenders.utils.python_evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_results(ranking_eval):\n",
    "    metrics = {\n",
    "        \"Precision@k\": ranking_eval.precision_at_k(),\n",
    "        \"Recall@k\": ranking_eval.recall_at_k(),\n",
    "        \"NDCG@k\": ranking_eval.ndcg_at_k(),\n",
    "        \"Mean average precision\": ranking_eval.map_at_k()\n",
    "      \n",
    "    }\n",
    "    return metrics   \n",
    "\n",
    "def get_diversity_results(diversity_eval):\n",
    "    metrics = {\n",
    "        \"catalog_coverage\":diversity_eval.catalog_coverage(),\n",
    "        \"distributional_coverage\":diversity_eval.distributional_coverage(), \n",
    "        \"novelty\": diversity_eval.novelty(), \n",
    "        \"diversity\": diversity_eval.diversity(), \n",
    "        \"serendipity\": diversity_eval.serendipity()\n",
    "    }\n",
    "    return metrics \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(data, algo, k, ranking_metrics, diversity_metrics):\n",
    "    summary = {\"Data\": data, \"Algo\": algo, \"K\": k}\n",
    "\n",
    "    if ranking_metrics is None:\n",
    "        ranking_metrics = {           \n",
    "            \"Precision@k\": np.nan,\n",
    "            \"Recall@k\": np.nan,            \n",
    "            \"nDCG@k\": np.nan,\n",
    "            \"MAP\": np.nan,\n",
    "        }\n",
    "    summary.update(ranking_metrics)\n",
    "    summary.update(diversity_metrics)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "als_ranking_eval = SparkRankingEvaluation(\n",
    "    test, \n",
    "    top_all, \n",
    "    k = TOP_K, \n",
    "    col_user=COL_USER, \n",
    "    col_item=COL_TRACK,\n",
    "    col_rating=COL_COUNT, \n",
    "    col_prediction=\"prediction\",\n",
    "    relevancy_method=\"top_k\"\n",
    ")\n",
    "\n",
    "als_ranking_metrics = get_ranking_results(als_ranking_eval)\n",
    "\n",
    "als_diversity_eval = SparkDiversityEvaluation(\n",
    "    train_df = train, \n",
    "    reco_df = top_k_reco,\n",
    "    col_user = COL_USER, \n",
    "    col_item = COL_TRACK\n",
    ")\n",
    "\n",
    "als_diversity_metrics = get_diversity_results(als_diversity_eval)\n",
    "\n",
    "als_results = generate_summary(sample.count(), \"als\", TOP_K, als_ranking_metrics, als_diversity_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add more models create the another cell with the other model metrics and generate a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Data\", \"Algo\", \"K\", \"Precision@k\", \"Recall@k\", \"NDCG@k\", \"Mean average precision\",\"catalog_coverage\", \"distributional_coverage\",\"novelty\", \"diversity\", \"serendipity\" ]\n",
    "df_results = pd.DataFrame(columns=cols)\n",
    "\n",
    "# add the models results here\n",
    "df_results.loc[1] = als_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Algo</th>\n",
       "      <th>K</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Mean average precision</th>\n",
       "      <th>catalog_coverage</th>\n",
       "      <th>distributional_coverage</th>\n",
       "      <th>novelty</th>\n",
       "      <th>diversity</th>\n",
       "      <th>serendipity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9643</td>\n",
       "      <td>als</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017445</td>\n",
       "      <td>5.759459</td>\n",
       "      <td>9.050837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Data Algo   K  Precision@k  Recall@k  NDCG@k  Mean average precision  \\\n",
       "1  9643  als  10          0.0       0.0     0.0                     0.0   \n",
       "\n",
       "   catalog_coverage  distributional_coverage   novelty  diversity  serendipity  \n",
       "1          0.017445                 5.759459  9.050837        1.0     0.999933  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup spark instance\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsyskernel",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
