{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# cornac imports\n",
    "import cornac\n",
    "from recommenders.models.cornac.cornac_utils import predict_ranking\n",
    "from recommenders.utils.constants import SEED\n",
    "\n",
    "# spark imports\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import recommenders.evaluation.python_evaluation as py_eval\n",
    "\n",
    "# data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# recommenders imports\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.evaluation.spark_evaluation import SparkRankingEvaluation, SparkDiversityEvaluation\n",
    "from recommenders.utils.spark_utils import start_or_get_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System version: {sys.version}\")\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 50\n",
    "\n",
    "# Model parameters (BPR)\n",
    "NUM_FACTORS = 200\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Column names for the dataset\n",
    "COL_USER = \"user_id\"\n",
    "COL_TRACK = \"track_id\"\n",
    "COL_COUNT = \"playcount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Ranking and Diversity functions\n",
    "Usable for pySpark. To test with bpr need to use recommenders.utils.python_evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_results_spark(ranking_eval):\n",
    "    metrics = {\n",
    "        \"Precision@k\": ranking_eval.precision_at_k(),\n",
    "        \"Recall@k\": ranking_eval.recall_at_k(),\n",
    "        \"NDCG@k\": ranking_eval.ndcg_at_k(),\n",
    "        \"Mean average precision\": ranking_eval.map_at_k()\n",
    "      \n",
    "    }\n",
    "    return metrics   \n",
    "\n",
    "def get_ranking_results_python(test, top_k_rec_bpr):\n",
    "    metrics = {\n",
    "        \"Precision@k\": py_eval.precision_at_k(test, top_k_rec_bpr, \n",
    "                                col_user=COL_USER, \n",
    "                                col_item=COL_TRACK, \n",
    "                                col_prediction='prediction', \n",
    "                                k=TOP_K, \n",
    "                                relevancy_method=None),\n",
    "        \"Recall@k\": py_eval.recall_at_k(test, top_k_rec_bpr, \n",
    "                          col_user=COL_USER, \n",
    "                          col_item=COL_TRACK, \n",
    "                          col_prediction='prediction', \n",
    "                          k=TOP_K, \n",
    "                          relevancy_method=None),\n",
    "        \"NDCG@k\": py_eval.ndcg_at_k(test, top_k_rec_bpr, \n",
    "                      col_user=COL_USER, \n",
    "                      col_item=COL_TRACK, \n",
    "                      col_rating=COL_COUNT, \n",
    "                      col_prediction='prediction', \n",
    "                      k=TOP_K, \n",
    "                      relevancy_method=None),\n",
    "        \"Mean average precision\": py_eval.map(test, top_k_rec_bpr, \n",
    "               col_user=COL_USER, \n",
    "               col_item=COL_TRACK, \n",
    "               col_prediction='prediction', \n",
    "               k=TOP_K,\n",
    "               relevancy_method=None)\n",
    "      \n",
    "    }\n",
    "    return metrics \n",
    "\n",
    "def get_diversity_results_spark(diversity_eval):\n",
    "    metrics = {\n",
    "        \"novelty\": diversity_eval.novelty(), \n",
    "        \"diversity\": diversity_eval.diversity()\n",
    "    }\n",
    "    return metrics \n",
    "\n",
    "def get_diversity_results_python(train, top_k_rec_bpr):\n",
    "    metrics = {\n",
    "        \"novelty\": py_eval.novelty(train_df=train,\n",
    "                       reco_df=top_k_rec_bpr,\n",
    "                       col_user=COL_USER,\n",
    "                       col_item=COL_TRACK), \n",
    "        \"diversity\": py_eval.diversity(train_df=train,\n",
    "                           reco_df=top_k_rec_bpr,\n",
    "                           col_user=COL_USER,\n",
    "                           col_item=COL_TRACK)\n",
    "    }\n",
    "    return metrics \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(data, algo, k, ranking_metrics, diversity_metrics):\n",
    "    summary = {\"Data\": data, \"Algo\": algo, \"K\": k}\n",
    "\n",
    "    if ranking_metrics is None:\n",
    "        ranking_metrics = {           \n",
    "            \"Precision@k\": np.nan,\n",
    "            \"Recall@k\": np.nan,            \n",
    "            \"nDCG@k\": np.nan,\n",
    "            \"MAP\": np.nan,\n",
    "        }\n",
    "    summary.update(ranking_metrics)\n",
    "    summary.update(diversity_metrics)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\", config={'spark.local.dir': \"/home/manuel-albino/spark-temp\", 'spark.cleaner.ttl': \"true\"})\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset into pyspark DataFrame    \n",
    "test_listening_history = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../remappings/data/dataset/test_listening_history_OverEqual_50_Interactions.txt\")\n",
    "    \n",
    "train_listening_history = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../remappings/data/dataset/train_listening_history_OverEqual_50_Interactions.txt\")\n",
    "\n",
    "# Change columns to correct place (user_id, track_id, playcount)\n",
    "test_listening_history = test_listening_history.withColumn(\"track_id_temp\", test_listening_history.track_id).withColumn(\"user_id_temp\", test_listening_history.user_id)\n",
    "test_listening_history = test_listening_history.withColumn(\"track_id\", test_listening_history.user_id_temp).withColumn(\"user_id\", test_listening_history.track_id_temp)\n",
    "\n",
    "train_listening_history = train_listening_history.withColumn(\"track_id_temp\", train_listening_history.track_id).withColumn(\"user_id_temp\", train_listening_history.user_id)\n",
    "train_listening_history = train_listening_history.withColumn(\"track_id\", train_listening_history.user_id_temp).withColumn(\"user_id\", train_listening_history.track_id_temp)\n",
    "\n",
    "# key = old column, value = new column\n",
    "mapping = {\n",
    "    \"track_id\": COL_USER,\n",
    "    \"user_id\": COL_TRACK,\n",
    "    \"playcount\": COL_COUNT\n",
    "}\n",
    "\n",
    "test_listening_history = test_listening_history.select(*[F.col(old).alias(new) for old, new in mapping.items()])\n",
    "train_listening_history = train_listening_history.select(*[F.col(old).alias(new) for old, new in mapping.items()])\n",
    "\n",
    "test_listening_history.show(2, truncate=False)\n",
    "train_listening_history.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_listening_history, test_listening_history\n",
    "\n",
    "# alpha = 1 \n",
    "\n",
    "# # Transform playcount to confidence using the current alpha\n",
    "# train = train.withColumn(\"confidence\", 1 + alpha * F.log(1 + F.col(COL_COUNT))).drop(COL_COUNT)\n",
    "\n",
    "# train.show(10, truncate=False)\n",
    "\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS model creation with Confidence column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 1 \n",
    "\n",
    "# # Transform playcount to confidence using the current alpha\n",
    "# train_with_confidence = train.withColumn(\"confidence\", 1 + alpha * F.log(1 + F.col(COL_COUNT))).drop(COL_COUNT)\n",
    "\n",
    "# train_with_confidence.show(10, truncate=False)\n",
    "\n",
    "header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_TRACK,\n",
    "    \"ratingCol\": COL_COUNT,\n",
    "}\n",
    "\n",
    "als = ALS(userCol= COL_USER, itemCol= COL_TRACK, ratingCol=COL_COUNT, rank = 10, maxIter = 40, regParam = 0.05, alpha = 60.0, coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as train_time:\n",
    "    model = als.fit(train)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as test_time:\n",
    "\n",
    "    # Get the cross join of all user-item pairs and score them.\n",
    "    users = train.select(COL_USER).distinct()\n",
    "    items = train.select(COL_TRACK).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item)\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_TRACK] == train[COL_TRACK]),\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    top_all_als = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_COUNT}\"].isNull()) \\\n",
    "        .select('pred.' + COL_USER, 'pred.' + COL_TRACK, 'pred.' + \"prediction\")\n",
    "\n",
    "    # In Spark, transformations are lazy evaluation\n",
    "    # Use an action to force execute and measure the test time \n",
    "    top_all_als.cache().count()\n",
    "\n",
    "    # top k recommendations for each user\n",
    "    window = Window.partitionBy(COL_USER).orderBy(F.col(\"prediction\").desc())    \n",
    "\n",
    "    top_k_reco_als = top_all_als.select(\"*\", F.row_number().over(window).alias(\"rank\")).filter(F.col(\"rank\") <= TOP_K).drop(\"rank\")\n",
    "    \n",
    "    print(top_k_reco_als.count())\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_all_als.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_ranking_eval = SparkRankingEvaluation(\n",
    "    test, \n",
    "    top_all_als, \n",
    "    k = TOP_K, \n",
    "    col_user=COL_USER, \n",
    "    col_item=COL_TRACK,\n",
    "    col_rating=COL_COUNT, \n",
    "    col_prediction=\"prediction\",\n",
    "    relevancy_method=\"top_k\"\n",
    ")\n",
    "\n",
    "als_ranking_metrics = get_ranking_results_spark(als_ranking_eval)\n",
    "\n",
    "als_diversity_eval = SparkDiversityEvaluation(\n",
    "    train_df = train, \n",
    "    reco_df = top_k_reco_als,\n",
    "    col_user = COL_USER, \n",
    "    col_item = COL_TRACK\n",
    ")\n",
    "\n",
    "als_diversity_metrics = get_diversity_results_spark(als_diversity_eval)\n",
    "\n",
    "als_results = generate_summary(train.count() + test.count(), \"als\", TOP_K, als_ranking_metrics, als_diversity_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPR & NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from file\n",
    "test_listening_history = pd.read_csv(header=0, delimiter=\"\\t\", filepath_or_buffer=\"../remappings/data/dataset/test_listening_history_OverEqual_50_Interactions.txt\")\n",
    "train_listening_history = pd.read_csv(header=0, delimiter=\"\\t\", filepath_or_buffer=\"../remappings/data/dataset/train_listening_history_OverEqual_50_Interactions.txt\")\n",
    "\n",
    "# Change columns to correct place (user_id, track_id, playcount)\n",
    "track_test = test_listening_history[\"track_id\"]\n",
    "user_test = test_listening_history[\"user_id\"]\n",
    "\n",
    "track_train = train_listening_history[\"track_id\"]\n",
    "user_train = train_listening_history[\"user_id\"]\n",
    "\n",
    "test_listening_history[\"track_id\"] = user_test\n",
    "test_listening_history[\"user_id\"] = track_test\n",
    "\n",
    "train_listening_history[\"track_id\"] = user_train\n",
    "train_listening_history[\"user_id\"] = track_train\n",
    "\n",
    "test_listening_history.columns = [COL_USER, COL_TRACK, COL_COUNT]\n",
    "train_listening_history.columns = [COL_USER, COL_TRACK, COL_COUNT]\n",
    "\n",
    "train, test = train_listening_history, test_listening_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Cornac Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)\n",
    "\n",
    "print('Number of users: {}'.format(train_set.num_users))\n",
    "print('Number of items: {}'.format(train_set.num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPR Model Train and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr = cornac.models.BPR(\n",
    "    k=NUM_FACTORS,\n",
    "    max_iter=NUM_EPOCHS,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.001,\n",
    "    verbose=True,\n",
    "    seed=SEED)\n",
    "\n",
    "with Timer() as t:\n",
    "    bpr.fit(train_set)\n",
    "print(\"Took {} seconds for training.\".format(t))\n",
    "\n",
    "\n",
    "with Timer() as t:\n",
    "    all_predictions_bpr = predict_ranking(bpr, train, usercol=COL_USER, itemcol=COL_TRACK, remove_seen=True)\n",
    "print(\"Took {} seconds for prediction.\".format(t))\n",
    "\n",
    "all_predictions_bpr.head()\n",
    "\n",
    "# Sort by 'user' and 'prediction' in descending order\n",
    "all_prediction_sorted_bpr = all_predictions_bpr.sort_values(by=[COL_USER, 'prediction'], ascending=[True, False])\n",
    "\n",
    "# Select the top k predictions for each user\n",
    "top_k_rec_bpr = all_prediction_sorted_bpr.groupby(COL_USER).head(TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPR metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_ranking_metrics = get_ranking_results_python(test, top_k_rec_bpr)\n",
    "\n",
    "bpr_diversity_metrics = get_diversity_results_python(train,top_k_rec_bpr)\n",
    "\n",
    "bpr_results = generate_summary(train.size + test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add more models create the another cell with the other model metrics and generate a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Data\", \"Algo\", \"K\", \"Precision@k\", \"Recall@k\", \"NDCG@k\", \"Mean average precision\",\"novelty\", \"diversity\"]\n",
    "df_results = pd.DataFrame(columns=cols)\n",
    "\n",
    "# add the models results here\n",
    "df_results.loc[1] = als_results \n",
    "df_results.loc[2] = bpr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup spark instance\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsyskernel",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
