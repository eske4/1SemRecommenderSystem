{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking (BPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.20 (main, Oct  3 2024, 07:27:41) \n",
      "[GCC 11.2.0]\n",
      "Cornac version: 1.18\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cornac\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.evaluation.python_evaluation import map, ndcg_at_k, precision_at_k, recall_at_k, diversity, novelty, serendipity, catalog_coverage, distributional_coverage\n",
    "from recommenders.models.cornac.cornac_utils import predict_ranking, predict\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "\n",
    "#from lenskit.metrics.topn import ndcg, precision, recall\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Cornac version: {cornac.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 50\n",
    "\n",
    "# Model parameters\n",
    "NUM_FACTORS = 200\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Column names for the dataset\n",
    "COL_USER = \"user\"\n",
    "COL_TRACK = \"item\"\n",
    "COL_COUNT = \"playcount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from file\n",
    "test_listening_history = pd.read_csv(header=0, delimiter=\"\\t\", filepath_or_buffer=\"../remappings/data/dataset/test_listening_history_OverEqual_50_Interactions.txt\")\n",
    "train_listening_history = pd.read_csv(header=0, delimiter=\"\\t\", filepath_or_buffer=\"../remappings/data/dataset/train_listening_history_OverEqual_50_Interactions.txt\")\n",
    "\n",
    "# Change columns to correct place (user_id, track_id, playcount)\n",
    "track_test = test_listening_history[\"track_id\"]\n",
    "user_test = test_listening_history[\"user_id\"]\n",
    "\n",
    "track_train = train_listening_history[\"track_id\"]\n",
    "user_train = train_listening_history[\"user_id\"]\n",
    "\n",
    "test_listening_history[\"track_id\"] = user_test\n",
    "test_listening_history[\"user_id\"] = track_test\n",
    "\n",
    "train_listening_history[\"track_id\"] = user_train\n",
    "train_listening_history[\"user_id\"] = track_train\n",
    "\n",
    "\n",
    "test_listening_history.columns = [COL_USER, COL_TRACK, COL_COUNT]\n",
    "train_listening_history.columns = [COL_USER, COL_TRACK, COL_COUNT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_listening_history, test_listening_history\n",
    "\n",
    "# Set the alpha value for the confidence transformation\n",
    "alpha = 1\n",
    "\n",
    "# Transform playcount to confidence in the training data only\n",
    "train[\"confidence\"] = 1 + alpha * np.log(1 + train[COL_COUNT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Cornac Dataset\n",
    "\n",
    "To work with models implemented in Cornac, we need to construct an object from [Dataset](https://cornac.readthedocs.io/en/latest/data.html#module-cornac.data.dataset) class.\n",
    "\n",
    "Dataset Class in Cornac serves as the main object that the models will interact with.  In addition to data transformations, Dataset provides a bunch of useful iterators for looping through the data, as well as supporting different negative sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 6066\n",
      "Number of items: 4287\n"
     ]
    }
   ],
   "source": [
    "train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)\n",
    "\n",
    "print('Number of users: {}'.format(train_set.num_users))\n",
    "print('Number of items: {}'.format(train_set.num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the BPR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr = cornac.models.BPR(\n",
    "    k=NUM_FACTORS,\n",
    "    max_iter=NUM_EPOCHS,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.001,\n",
    "    verbose=True,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 140.56it/s, correct=65.71%, skipped=0.01%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished!\n",
      "Took 0.7473 seconds for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    bpr.fit(train_set)\n",
    "print(\"Took {} seconds for training.\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 27.1137 seconds for prediction.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.397102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.208740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>1.340856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>1.427171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>0.232193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  prediction\n",
       "0    11     1   -0.397102\n",
       "1    11    27   -0.208740\n",
       "2    11    43    1.340856\n",
       "3    11    44    1.427171\n",
       "4    11    45    0.232193"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    all_predictions = predict_ranking(bpr, train, usercol=COL_USER, itemcol=COL_TRACK, remove_seen=True)\n",
    "print(\"Took {} seconds for prediction.\".format(t))\n",
    "\n",
    "all_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'user' and 'prediction' in descending order\n",
    "all_prediction_sorted = all_predictions.sort_values(by=[COL_USER, 'prediction'], ascending=[True, False])\n",
    "\n",
    "# Select the top k predictions for each user\n",
    "top_k_rec = all_prediction_sorted.groupby(COL_USER).head(TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel-albino/miniconda3/envs/recsys/lib/python3.9/site-packages/recommenders/evaluation/python_evaluation.py:438: FutureWarning: Passing a dictionary to SeriesGroupBy.agg is deprecated and will raise in a future version of pandas. Pass a list of aggregations instead.\n",
      "  df_hit.groupby(col_user, as_index=False)[col_user].agg({\"hit\": \"count\"}),\n",
      "/home/manuel-albino/miniconda3/envs/recsys/lib/python3.9/site-packages/recommenders/evaluation/python_evaluation.py:439: FutureWarning: Passing a dictionary to SeriesGroupBy.agg is deprecated and will raise in a future version of pandas. Pass a list of aggregations instead.\n",
      "  rating_true_common.groupby(col_user, as_index=False)[col_user].agg(\n",
      "/home/manuel-albino/miniconda3/envs/recsys/lib/python3.9/site-packages/recommenders/evaluation/python_evaluation.py:1348: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  avg_diversity = df_user_diversity.agg({\"user_diversity\": \"mean\"})[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K Spark:\t0.000883\n",
      "Recall@K Spark:\t0.044146\n",
      "NDCG Spark:\t0.003712\n",
      "Diversity Spark:\t0.999201\n",
      "Novelty Spark:\t9.628086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel-albino/miniconda3/envs/recsys/lib/python3.9/site-packages/recommenders/evaluation/python_evaluation.py:1435: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  avg_novelty = reco_item_novelty.agg({\"product\": \"sum\"})[0] / n_recommendations\n"
     ]
    }
   ],
   "source": [
    "eval_map = map(test, all_predictions, col_user=COL_USER, col_item=COL_TRACK, col_prediction='prediction', k=TOP_K)\n",
    "eval_ndcg = ndcg_at_k(test, all_predictions, col_user=COL_USER, col_item=COL_TRACK, col_rating=COL_COUNT, col_prediction='prediction', k=TOP_K)\n",
    "eval_precision = precision_at_k(test, all_predictions, col_user=COL_USER, col_item=COL_TRACK, col_prediction='prediction', k=TOP_K)\n",
    "eval_recall = recall_at_k(test, all_predictions, col_user=COL_USER, col_item=COL_TRACK, col_prediction='prediction', k=TOP_K)\n",
    "eval_diversity = diversity(\n",
    "    train_df=train,\n",
    "    reco_df=top_k_rec,\n",
    "    col_user=COL_USER,\n",
    "    col_item=COL_TRACK\n",
    ")\n",
    "eval_novelty = novelty(\n",
    "    train_df=train,\n",
    "    reco_df=top_k_rec,\n",
    "    col_user=COL_USER,\n",
    "    col_item=COL_TRACK\n",
    ")\n",
    "# missing serendipity, catalog_coverage and distributional_coverage to be equal to the als metrics\n",
    "# may be incorrect\n",
    "\n",
    "# Print evaluation metrics, including diversity\n",
    "print(\"Precision@K Spark:\\t%f\" % eval_precision,\n",
    "      \"Recall@K Spark:\\t%f\" % eval_recall,\n",
    "      \"NDCG Spark:\\t%f\" % eval_ndcg,\n",
    "      \"Diversity Spark:\\t%f\" % eval_diversity,\n",
    "      \"Novelty Spark:\\t%f\" % eval_novelty, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K Lenskit:\t0.000000\n",
      "Recall@K Lenskit:\t0.000000\n",
      "NDCG Lenskit:\t0.000000\n"
     ]
    }
   ],
   "source": [
    "# eval_ndcg_lens = ndcg(all_predictions, test, k=TOP_K)\n",
    "# eval_precision_lens = precision(all_predictions, test, k=TOP_K)\n",
    "# eval_recall_lens = recall(all_predictions, test, k=TOP_K)\n",
    "\n",
    "# # Print evaluation metrics, including diversity\n",
    "# print(\"Precision@K Lenskit:\\t%f\" % eval_precision_lens,\n",
    "#       \"Recall@K Lenskit:\\t%f\" % eval_recall_lens,\n",
    "#       \"NDCG Lenskit:\\t%f\" % eval_ndcg_lens, sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsyskernel",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
