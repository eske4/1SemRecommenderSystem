{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# spark imports\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "# data science imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# recommenders imports\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.notebook_utils import is_jupyter\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation, SparkDiversityEvaluation\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.utils.notebook_utils import store_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.20 (main, Oct  3 2024, 07:27:41) \n",
      "[GCC 11.2.0]\n",
      "Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"System version: {sys.version}\")\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Column names for the dataset\n",
    "COL_USER = \"user_id\"\n",
    "COL_TRACK = \"track_id\"\n",
    "COL_COUNT = \"playcount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 17:24:54 WARN Utils: Your hostname, manuel-albino-asus resolves to a loopback address: 127.0.1.1; using 192.168.87.143 instead (on interface wlp1s0)\n",
      "24/11/06 17:24:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/06 17:24:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/06 17:24:55 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\", config={'spark.local.dir': \"/home/manuel-albino/spark-temp\", 'spark.cleaner.ttl': \"true\"})\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "|user_id|track_id|playcount|\n",
      "+-------+--------+---------+\n",
      "|20     |19508   |5        |\n",
      "|55     |1524    |1        |\n",
      "+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in the dataset into pyspark DataFrame\n",
    "song_ratings = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../remappings/data/Modified_Listening_History.txt\")\n",
    "\n",
    "#remapping\n",
    "song_ratings = song_ratings.withColumn(\"track_id_temp\", song_ratings.track_id).withColumn(\"user_id_temp\", song_ratings.user_id)\n",
    "\n",
    "song_ratings = song_ratings.withColumn(\"track_id\", song_ratings.user_id_temp).withColumn(\"user_id\", song_ratings.track_id_temp)\n",
    "\n",
    "# key = old column, value = new column\n",
    "mapping = {\n",
    "    \"track_id\": COL_USER,\n",
    "    \"user_id\": COL_TRACK,\n",
    "    \"playcount\": COL_COUNT\n",
    "}\n",
    "\n",
    "song_ratings = song_ratings.select(*[F.col(old).alias(new) for old, new in mapping.items()])\n",
    "sample = song_ratings.sample(False, 0.002, 0)\n",
    "\n",
    "# show matrix (track, user, playcount)\n",
    "sample.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 14578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:======================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N test 4829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(sample, ratio=0.75, seed=123)\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+------------------+\n",
      "|user_id|track_id|playcount|confidence        |\n",
      "+-------+--------+---------+------------------+\n",
      "|20     |19508   |5        |2.791759469228055 |\n",
      "|55     |1524    |1        |1.6931471805599454|\n",
      "|233    |2066    |1        |1.6931471805599454|\n",
      "|263    |20555   |7        |3.0794415416798357|\n",
      "|343    |30642   |1        |1.6931471805599454|\n",
      "|374    |11074   |11       |3.4849066497880004|\n",
      "|419    |7308    |1        |1.6931471805599454|\n",
      "|446    |8652    |4        |2.6094379124341005|\n",
      "|503    |28518   |3        |2.386294361119891 |\n",
      "|533    |2198    |1        |1.6931471805599454|\n",
      "+-------+--------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 1 \n",
    "\n",
    "# Transform playcount to confidence using the current alpha\n",
    "train_with_confidence = train.withColumn(\"confidence\", 1 + alpha * F.log(1 + F.col(COL_COUNT)))\n",
    "\n",
    "train_with_confidence.show(10, truncate=False)\n",
    "\n",
    "header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_TRACK,\n",
    "    \"ratingCol\": \"confidence\",\n",
    "}\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=True,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=42,\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 5.531200724000001 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = als.fit(train_with_confidence)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 17:25:15 WARN Column: Constructing trivially true equals predicate, 'user_id#47 = user_id#47'. Perhaps you need to use aliases.\n",
      "24/11/06 17:25:15 WARN Column: Constructing trivially true equals predicate, 'track_id#48 = track_id#48'. Perhaps you need to use aliases.\n",
      "[Stage 726:===================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 104.003172974 seconds for prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "\n",
    "    # Get the cross join of all user-item pairs and score them.\n",
    "    users = train.select(COL_USER).distinct()\n",
    "    items = train.select(COL_TRACK).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item)\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_TRACK] == train[COL_TRACK]),\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_COUNT}\"].isNull()) \\\n",
    "        .select('pred.' + COL_USER, 'pred.' + COL_TRACK, 'pred.' + \"prediction\")\n",
    "\n",
    "    # In Spark, transformations are lazy evaluation\n",
    "    # Use an action to force execute and measure the test time \n",
    "    top_all.cache().count()\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+\n",
      "|user_id|track_id|    prediction|\n",
      "+-------+--------+--------------+\n",
      "|     20|    2909|  -5.01851E-30|\n",
      "|     20|    2926|-2.6883428E-30|\n",
      "|     20|    4069|-3.0519938E-22|\n",
      "|     20|    4304|-1.7599168E-32|\n",
      "|     20|    5365| -5.594408E-34|\n",
      "|     20|    7844| -1.759787E-31|\n",
      "|     20|   11109|  8.204271E-34|\n",
      "|     20|   11839| 6.3963253E-27|\n",
      "|     20|   12386|  9.030782E-26|\n",
      "|     20|   12565| 2.4726398E-27|\n",
      "|     20|   13098|-3.0713183E-34|\n",
      "|     20|   15321|  6.230454E-26|\n",
      "|     20|   16849| -2.459086E-34|\n",
      "|     20|   19238|  8.586324E-35|\n",
      "|     20|   22071|  2.383432E-15|\n",
      "|     20|   23142|  2.513782E-28|\n",
      "|     20|   23407| 1.0425959E-27|\n",
      "|     20|   28345| -1.627138E-27|\n",
      "|     20|   29045|-5.5587636E-35|\n",
      "|     20|   29650| 2.4267886E-25|\n",
      "+-------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_TRACK, \n",
    "                                    col_rating=COL_COUNT, col_prediction=\"prediction\", \n",
    "                                    relevancy_method=\"top_k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 767:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS\n",
      "Top K:\t10\n",
      "MAP:\t0.003883\n",
      "NDCG:\t0.006759\n",
      "Precision@K:\t0.001604\n",
      "Recall@K:\t0.016043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\tALS\",\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+--------------+\n",
      "|user_id|track_id|playcount|    prediction|\n",
      "+-------+--------+---------+--------------+\n",
      "| 418759|    2066|        1|  7.785207E-21|\n",
      "| 658503|     419|        1| -8.126021E-12|\n",
      "| 567911|     579|        3|-1.1795405E-20|\n",
      "| 575724|   41043|        1| 2.0479983E-18|\n",
      "| 610369|    1168|        1|-1.9749431E-19|\n",
      "| 739634|    6029|        3| -4.893929E-22|\n",
      "| 944682|    1675|        4| 1.1564163E-11|\n",
      "| 758122|   34815|        1| 1.5832767E-28|\n",
      "| 625076|    1207|        2| 1.1859326E-29|\n",
      "| 771332|    3008|        1|-3.1439335E-20|\n",
      "| 231671|   43873|        1| 1.2213541E-15|\n",
      "| 720057|    1312|        1|  9.408535E-26|\n",
      "| 163936|    2742|        1| -2.4290284E-7|\n",
      "| 402350|    4196|       11| -6.222404E-25|\n",
      "| 817602|    3002|        1|  6.627791E-39|\n",
      "| 118913|    9894|        1|-1.2196397E-28|\n",
      "| 330928|    5310|        1|-2.1665012E-24|\n",
      "| 930111|   14022|        1|-1.2064258E-37|\n",
      "| 343301|   40218|        1| 6.1238575E-27|\n",
      "| 417440|   24806|        1| 6.2614767E-17|\n",
      "+-------+--------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predicted ratings.\n",
    "prediction = model.transform(test)\n",
    "prediction.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS rating prediction\n",
      "RMSE:\t4.253323\n",
      "MAE:\t2.161777\n",
      "Explained variance:\t-0.000004\n",
      "R squared:\t-0.348303\n"
     ]
    }
   ],
   "source": [
    "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_TRACK, \n",
    "                                    col_rating=COL_COUNT, col_prediction=\"prediction\")\n",
    "\n",
    "print(\"Model:\\tALS rating prediction\",\n",
    "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
    "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
    "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
    "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup spark instance\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsyskernel",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
