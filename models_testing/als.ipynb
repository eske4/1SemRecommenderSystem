{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# spark imports\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# recommenders imports\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.evaluation.spark_evaluation import SparkRankingEvaluation, SparkDiversityEvaluation\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System version: {sys.version}\")\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 50\n",
    "\n",
    "# Column names for the dataset\n",
    "COL_USER = \"user_id\"\n",
    "COL_TRACK = \"track_id\"\n",
    "COL_COUNT = \"playcount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\", config={'spark.local.dir': \"/home/matildeschade/spark-temp\", 'spark.cleaner.ttl': \"true\"})\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset into pyspark DataFrame    \n",
    "test_listening_history = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../remappings/data/dataset/test_listening_history_OverEqual_50_Interactions.txt\")\n",
    "    \n",
    "train_listening_history = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../remappings/data/dataset/train_listening_history_OverEqual_50_Interactions.txt\")\n",
    "\n",
    "# Change columns to correct place (user_id, track_id, playcount)\n",
    "test_listening_history = test_listening_history.withColumn(\"track_id_temp\", test_listening_history.track_id).withColumn(\"user_id_temp\", test_listening_history.user_id)\n",
    "test_listening_history = test_listening_history.withColumn(\"track_id\", test_listening_history.user_id_temp).withColumn(\"user_id\", test_listening_history.track_id_temp)\n",
    "\n",
    "train_listening_history = train_listening_history.withColumn(\"track_id_temp\", train_listening_history.track_id).withColumn(\"user_id_temp\", train_listening_history.user_id)\n",
    "train_listening_history = train_listening_history.withColumn(\"track_id\", train_listening_history.user_id_temp).withColumn(\"user_id\", train_listening_history.track_id_temp)\n",
    "\n",
    "# key = old column, value = new column\n",
    "mapping = {\n",
    "    \"track_id\": COL_USER,\n",
    "    \"user_id\": COL_TRACK,\n",
    "    \"playcount\": COL_COUNT\n",
    "}\n",
    "\n",
    "test_listening_history = test_listening_history.select(*[F.col(old).alias(new) for old, new in mapping.items()])\n",
    "train_listening_history = train_listening_history.select(*[F.col(old).alias(new) for old, new in mapping.items()])\n",
    "\n",
    "# Sample\n",
    "test_listening_history = test_listening_history.sample(False, 0.005, 0)\n",
    "train_listening_history = train_listening_history.sample(False, 0.005, 0)\n",
    "\n",
    "test_listening_history.show(2, truncate=False)\n",
    "train_listening_history.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_listening_history, test_listening_history\n",
    "\n",
    "# alpha = 1 \n",
    "\n",
    "# # Transform playcount to confidence using the current alpha\n",
    "# train = train.withColumn(\"confidence\", 1 + alpha * F.log(1 + F.col(COL_COUNT))).drop(COL_COUNT)\n",
    "\n",
    "# train.show(10, truncate=False)\n",
    "\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train de ALS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify ALS hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [10, 20, 30, 40]\n",
    "maxIters = [10, 20, 30, 40]\n",
    "regParams = [.05, .1, .15]\n",
    "alphas = [20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop will automatically create and store ALS models\n",
    "model_list = []\n",
    "\n",
    "for r in tqdm(ranks):\n",
    "    for mi in maxIters:\n",
    "        for rp in regParams:\n",
    "            for a in alphas:\n",
    "                model_list.append(ALS(userCol= COL_USER, itemCol= COL_TRACK, ratingCol=COL_COUNT, rank = r, maxIter = mi, regParam = rp, alpha = a, coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = True))\n",
    "\n",
    "# Print the model list, and the length of model_list\n",
    "print (model_list, \"Length of model_list: \", len(model_list))\n",
    "\n",
    "# Validate\n",
    "len(model_list) == (len(ranks)*len(maxIters)*len(regParams)*len(alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected percentile rank error metric function\n",
    "def ROEM(predictions, userCol = COL_USER, itemCol = COL_TRACK, ratingCol = COL_COUNT):\n",
    "  # Creates table that can be queried\n",
    "  predictions.createOrReplaceTempView(\"predictions\")\n",
    "  \n",
    "  # Sum of total number of plays of all songs\n",
    "  denominator = predictions.groupBy().sum(ratingCol).collect()[0][0]\n",
    "\n",
    "  # Calculating rankings of songs predictions by user\n",
    "  spark.sql(\"SELECT \" + userCol + \" , \" + ratingCol + \" , PERCENT_RANK() OVER (PARTITION BY \" + userCol + \" ORDER BY prediction DESC) AS rank FROM predictions\").createOrReplaceTempView(\"rankings\")\n",
    "\n",
    "  # Multiplies the rank of each song by the number of plays and adds the products together\n",
    "  numerator = spark.sql('SELECT SUM(' + ratingCol + ' * rank) FROM rankings').collect()[0][0]\n",
    "  performance = numerator/denominator\n",
    "  \n",
    "  return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building 5 folds within the training set.\n",
    "# train1, train2, train3, train4, train5 = train.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 1)\n",
    "# fold1 = train2.union(train3).union(train4).union(train5)\n",
    "# fold2 = train3.union(train4).union(train5).union(train1)\n",
    "# fold3 = train4.union(train5).union(train1).union(train2)\n",
    "# fold4 = train5.union(train1).union(train2).union(train3)\n",
    "# fold5 = train1.union(train2).union(train3).union(train4)\n",
    "\n",
    "# foldlist = [(fold1, train1), (fold2, train2), (fold3, train3), (fold4, train4), (fold5, train5)]\n",
    "\n",
    "# # Empty list to fill with ROEMs from each model\n",
    "# ROEMS = []\n",
    "\n",
    "# # Loops through all models and all folds\n",
    "# for model in model_list:\n",
    "#     for ft_pair in foldlist:\n",
    "\n",
    "#         # Fits model to fold within training data\n",
    "#         fitted_model = model.fit(ft_pair[0])\n",
    "\n",
    "#         # Generates predictions using fitted_model on respective CV test data\n",
    "#         predictions = fitted_model.transform(ft_pair[1])\n",
    "\n",
    "#         # Generates and prints a ROEM metric CV test data\n",
    "#         r = ROEM(predictions)\n",
    "#         print (\"ROEM: \", r)\n",
    "\n",
    "#     # Fits model to all of training data and generates preds for test data\n",
    "#     v_fitted_model = model.fit(train)\n",
    "#     v_predictions = v_fitted_model.transform(test)\n",
    "#     v_ROEM = ROEM(v_predictions)\n",
    "\n",
    "#     # Adds validation ROEM to ROEM list\n",
    "#     ROEMS.append(v_ROEM)\n",
    "#     print (\"Validation ROEM: \", v_ROEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best_model\n",
    "best_model = model_list[38]\n",
    "\n",
    "# Extract the Rank\n",
    "best_rank = best_model.getRank()\n",
    "print (\"Rank: \", best_rank)\n",
    "\n",
    "# Extract the MaxIter value\n",
    "best_maxIter = best_model.getMaxIter()\n",
    "print (\"MaxIter: \", best_maxIter)\n",
    "\n",
    "# Extract the RegParam value\n",
    "best_regParam = best_model.getRegParam()\n",
    "print (\"RegParam: \", best_regParam)\n",
    "\n",
    "# Extract the Alpha value\n",
    "best_alpha = best_model.getAlpha()\n",
    "print (\"Alpha: \", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header = {\n",
    "#     \"userCol\": COL_USER,\n",
    "#     \"itemCol\": COL_TRACK,\n",
    "#     \"ratingCol\": 'confidence',\n",
    "# }\n",
    "\n",
    "# als = ALS(\n",
    "#     rank=best_rank,\n",
    "#     maxIter=best_maxIter,\n",
    "#     implicitPrefs=True,\n",
    "#     regParam=best_regParam,\n",
    "#     alpha=best_alpha,\n",
    "#     coldStartStrategy='drop',\n",
    "#     nonnegative=True,\n",
    "#     seed=42,\n",
    "#     **header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as train_time:\n",
    "    model = best_model.fit(train)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as test_time:\n",
    "\n",
    "    # Get the cross join of all user-item pairs and score them.\n",
    "    users = train.select(COL_USER).distinct()\n",
    "    items = train.select(COL_TRACK).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item)\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_TRACK] == train[COL_TRACK]),\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_COUNT}\"].isNull()) \\\n",
    "        .select('pred.' + COL_USER, 'pred.' + COL_TRACK, 'pred.' + \"prediction\")\n",
    "\n",
    "    # In Spark, transformations are lazy evaluation\n",
    "    # Use an action to force execute and measure the test time \n",
    "    top_all.cache().count()\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k_rec = model.recommendForAllUsers(TOP_K)\n",
    "# top_k_rec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Timer() as t:\n",
    "#     predictions = model.transform(test)\n",
    "# print(\"Took {} seconds for prediction.\".format(t))\n",
    "\n",
    "# predictions.cache().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_TRACK, \n",
    "                                    col_rating=COL_COUNT, col_prediction=\"prediction\", relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model:\\tALS\",\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), \n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup spark instance\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsyskernel",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
